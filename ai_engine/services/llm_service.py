# llm_service/gemma_client.py
# Client optimisÃ© pour interaction avec Gemma3 via Ollama
# Support des prompts CV/Offre avec gestion d'erreurs robuste

from langchain_ollama import OllamaLLM
from typing import Optional, Dict, List, Any
import json
import time
import logging
import re
from dataclasses import dataclass
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

@dataclass
class LLMResponse:
    """RÃ©ponse structurÃ©e du LLM avec mÃ©tadonnÃ©es"""
    content: str
    tokens_used: int
    execution_time: float
    model_used: str
    temperature: float
    success: bool
    error: Optional[str] = None
    retries_used: int = 0

class GemmaClient:
    """
    Client optimisÃ© pour Gemma3 avec retry logic et validation
    SpÃ©cialement conÃ§u pour parsing CV et offres d'emploi
    """
    
    def __init__(self, 
                 model_name: str = "gemma3:4b",
                 base_url: str = "http://localhost:11434",
                 temperature: float = 0.1,
                 max_retries: int = 3,
                 timeout: int = 120,
                 log_level: str = "INFO"):
        
        self.model_name = model_name
        self.base_url = base_url
        self.temperature = temperature
        self.max_retries = max_retries
        self.timeout = timeout
        
        self.setup_logging(log_level)
        self._initialize_client()
        
        # MÃ©triques de performance
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "average_response_time": 0.0
        }
        
    def setup_logging(self, log_level: str):
        """Configuration du logging"""
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('GemmaClient')
    
    def _initialize_client(self):
        """Initialisation du client Ollama avec validation"""
        try:
            self.llm = OllamaLLM(
                model=self.model_name,
                base_url=self.base_url,
                temperature=self.temperature,
                timeout=self.timeout
            )
            
            # Test de connexion
            self._test_connection()
            self.logger.info(f"Client Gemma3 initialisÃ©: {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"Erreur initialisation client: {str(e)}")
            raise Exception(f"Impossible d'initialiser le client Gemma3: {str(e)}")
    
    def _test_connection(self):
        """Test de connexion avec le modÃ¨le"""
        try:
            test_prompt = "RÃ©ponds simplement: OK"
            response = self.llm.invoke(test_prompt)
            
            if not response or len(response.strip()) == 0:
                raise Exception("RÃ©ponse vide du modÃ¨le")
                
            self.logger.info("Test de connexion rÃ©ussi")
            
        except Exception as e:
            raise Exception(f"Test de connexion Ã©chouÃ©: {str(e)}")
    
    def invoke(self, prompt: str, **kwargs) -> LLMResponse:
        """
        Invocation robuste avec retry et mÃ©triques
        
        Args:
            prompt: Le prompt Ã  envoyer
            **kwargs: Arguments supplÃ©mentaires (temperature, etc.)
            
        Returns:
            LLMResponse avec contenu et mÃ©tadonnÃ©es
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        # Override des paramÃ¨tres si fournis
        temp = kwargs.get('temperature', self.temperature)
        
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"Invocation tentative {attempt + 1}/{self.max_retries}")
                
                # Configuration temporaire si diffÃ©rente
                if temp != self.temperature:
                    original_temp = self.llm.temperature
                    self.llm.temperature = temp
                
                # Invocation
                response_content = self.llm.invoke(prompt)
                
                # Restauration tempÃ©rature originale
                if temp != self.temperature:
                    self.llm.temperature = original_temp
                
                # Validation de la rÃ©ponse
                if not response_content or len(response_content.strip()) == 0:
                    raise Exception("RÃ©ponse vide du modÃ¨le")
                
                execution_time = time.time() - start_time
                tokens_estimate = self._estimate_tokens(prompt + response_content)
                
                # Mise Ã  jour des statistiques
                self.stats["successful_requests"] += 1
                self.stats["total_tokens"] += tokens_estimate
                self.stats["total_time"] += execution_time
                self.stats["average_response_time"] = (
                    self.stats["total_time"] / self.stats["successful_requests"]
                )
                
                self.logger.info(f"RÃ©ponse reÃ§ue: {len(response_content)} chars en {execution_time:.2f}s")
                
                return LLMResponse(
                    content=response_content,
                    tokens_used=tokens_estimate,
                    execution_time=execution_time,
                    model_used=self.model_name,
                    temperature=temp,
                    success=True,
                    retries_used=attempt
                )
                
            except Exception as e:
                self.logger.warning(f"Tentative {attempt + 1} Ã©chouÃ©e: {str(e)}")
                
                if attempt == self.max_retries - 1:
                    # DerniÃ¨re tentative Ã©chouÃ©e
                    execution_time = time.time() - start_time
                    self.stats["failed_requests"] += 1
                    
                    return LLMResponse(
                        content="",
                        tokens_used=0,
                        execution_time=execution_time,
                        model_used=self.model_name,
                        temperature=temp,
                        success=False,
                        error=str(e),
                        retries_used=attempt + 1
                    )
                
                # Attente avant retry
                time.sleep(2 ** attempt)  # Backoff exponentiel
        
        # Ne devrait jamais arriver
        return LLMResponse(
            content="",
            tokens_used=0,
            execution_time=0,
            model_used=self.model_name,
            temperature=temp,
            success=False,
            error="Max retries exceeded"
        )
    
    def _estimate_tokens(self, text: str) -> int:
        """Estimation approximative du nombre de tokens"""
        # Estimation simple: ~4 caractÃ¨res par token pour le franÃ§ais
        return len(text) // 4
    
    def invoke_with_json_validation(self, prompt: str, **kwargs) -> LLMResponse:
        """
        Invocation avec validation JSON intÃ©grÃ©e
        IdÃ©al pour parsing CV/offres
        """
        response = self.invoke(prompt, **kwargs)
        
        if response.success:
            # Tentative d'extraction et validation JSON
            json_content = self._extract_json_from_response(response.content)
            if json_content:
                response.content = json_content
                self.logger.info("JSON valide extrait de la rÃ©ponse")
            else:
                response.success = False
                response.error = "Aucun JSON valide trouvÃ© dans la rÃ©ponse"
                self.logger.warning("RÃ©ponse ne contient pas de JSON valide")
        
        return response
    
    def _extract_json_from_response(self, text: str) -> Optional[str]:
        """Extrait et valide le JSON d'une rÃ©ponse"""
        if not text:
            return None
        
        # Patterns pour trouver du JSON
        json_patterns = [
            r'```json\s*(\{.*?\})\s*```',  # Markdown code block
            r'```\s*(\{.*?\})\s*```',      # Code block sans 'json'
            r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})',  # JSON simple
            r'(\[[^\[\]]*(?:\[[^\[\]]*\][^\[\]]*)*\])'  # Array JSON
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.MULTILINE)
            for match in matches:
                try:
                    # Validation JSON
                    json.loads(match)
                    return match
                except json.JSONDecodeError:
                    continue
        
        return None
    
    def create_cv_parsing_prompt(self, cv_text: str) -> str:
        """GÃ©nÃ¨re un prompt optimisÃ© pour parser un CV"""
        return f"""Tu es un expert en analyse de CV. Analyse le texte suivant et extrait toutes les informations dans un format JSON structurÃ©.

TEXTE DU CV:
{cv_text}

INSTRUCTIONS:
1. Extrait TOUTES les informations disponibles
2. Respecte EXACTEMENT le format JSON ci-dessous
3. Si une information n'est pas disponible, utilise une liste vide [] ou une chaÃ®ne vide ""
4. Pour l'expÃ©rience, compte le nombre total d'annÃ©es en additionnant toutes les expÃ©riences
5. Sois prÃ©cis sur les compÃ©tences techniques et langues

FORMAT JSON REQUIS:
```json
{{
    "titre_candidat": "Titre ou poste recherchÃ©",
    "profil_resume": "RÃ©sumÃ© professionnel ou objectif de carriÃ¨re",
    "formations": [
        {{"diplome": "Nom du diplÃ´me", "etablissement": "Ã‰cole/UniversitÃ©", "annee": 2020}}
    ],
    "experience_years": 5,
    "experience": [
        {{"poste": "Titre du poste", "entreprise": "Nom entreprise", "periode": "2020-2023", "missions": "Description des missions"}}
    ],
    "competences_techniques": ["Python", "Java", "React"],
    "competences_informatiques": ["Git", "Docker", "AWS"],
    "langues": [
        {{"langue": "franÃ§ais", "niveau": "C2"}},
        {{"langue": "anglais", "niveau": "B2"}}
    ],
    "certifications": ["Certification 1", "Certification 2"],
    "projets": [
        {{"nom": "Nom du projet", "description": "Description", "technologies": ["Tech1", "Tech2"]}}
    ],
    "soft_skills": ["Communication", "Leadership", "Travail en Ã©quipe"],
    "coordonnees": {{
        "email": "email@example.com",
        "telephone": "+33123456789",
        "localisation": "Ville, Pays"
    }}
}}
```

RÃ‰PONDS UNIQUEMENT AVEC LE JSON, RIEN D'AUTRE."""
    
    def create_job_parsing_prompt(self, job_text: str) -> str:
        """GÃ©nÃ¨re un prompt optimisÃ© pour parser une offre d'emploi"""
        return f"""Tu es un expert en analyse d'offres d'emploi. Analyse le texte suivant et extrait toutes les informations dans un format JSON structurÃ©.

TEXTE DE L'OFFRE:
{job_text}

INSTRUCTIONS:
1. Extrait TOUTES les informations disponibles
2. Respecte EXACTEMENT le format JSON ci-dessous
3. Si une information n'est pas disponible, utilise une liste vide [] ou une chaÃ®ne vide ""
4. Identifie clairement les compÃ©tences OBLIGATOIRES vs SOUHAITÃ‰ES
5. Sois prÃ©cis sur les exigences de formation et d'expÃ©rience

FORMAT JSON REQUIS:
```json
{{
    "titre_poste": "Titre exact du poste",
    "entreprise": "Nom de l'entreprise",
    "localisation": "Ville, RÃ©gion, Pays",
    "type_contrat": "CDI/CDD/Stage/Freelance",
    "salaire": "Fourchette salariale si mentionnÃ©e",
    "missions": "Description dÃ©taillÃ©e des missions et responsabilitÃ©s",
    "environnement": "Description de l'environnement de travail",
    "exigences": {{
        "formation_requise": "Niveau de formation requis",
        "annees_experience": 3,
        "competences_obligatoires": ["CompÃ©tence 1 OBLIGATOIRE", "CompÃ©tence 2 REQUISE"],
        "competences_souhaitees": ["CompÃ©tence 1 souhaitÃ©e", "CompÃ©tence 2 apprÃ©ciÃ©e"],
        "langues": ["franÃ§ais: C1", "anglais: B2"],
        "certifications": ["Certification requise 1", "Certification souhaitÃ©e 2"],
        "outils": ["Outil 1", "Logiciel 2", "Plateforme 3"],
        "qualites_humaines": ["QualitÃ© 1", "Soft skill 2"]
    }},
    "avantages": ["Avantage 1", "BÃ©nÃ©fice 2"],
    "process_recrutement": "Description du processus si mentionnÃ©",
    "date_publication": "Date de publication si trouvÃ©e",
    "secteur": "Secteur d'activitÃ©"
}}
```

RÃ‰PONDS UNIQUEMENT AVEC LE JSON, RIEN D'AUTRE."""
    
    def parse_cv_text(self, cv_text: str) -> Dict:
        """
        Parse un texte de CV et retourne un dictionnaire structurÃ©
        
        Args:
            cv_text: Texte brut du CV
            
        Returns:
            Dict: CV structurÃ© ou erreur
        """
        if not cv_text or len(cv_text.strip()) < 50:
            return {"error": "Texte de CV trop court ou vide"}
        
        prompt = self.create_cv_parsing_prompt(cv_text)
        response = self.invoke_with_json_validation(prompt, temperature=0.1)
        
        if not response.success:
            self.logger.error(f"Erreur parsing CV: {response.error}")
            return {"error": f"Ã‰chec du parsing: {response.error}"}
        
        try:
            cv_data = json.loads(response.content)
            cv_data["_metadata"] = {
                "parsing_time": response.execution_time,
                "tokens_used": response.tokens_used,
                "model_used": response.model_used,
                "confidence": self._calculate_parsing_confidence(cv_data)
            }
            return cv_data
            
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON invalide reÃ§u: {str(e)}")
            return {"error": f"JSON invalide: {str(e)}"}
    
    def parse_job_text(self, job_text: str) -> Dict:
        """
        Parse un texte d'offre d'emploi et retourne un dictionnaire structurÃ©
        
        Args:
            job_text: Texte brut de l'offre
            
        Returns:
            Dict: Offre structurÃ©e ou erreur
        """
        if not job_text or len(job_text.strip()) < 50:
            return {"error": "Texte d'offre trop court ou vide"}
        
        prompt = self.create_job_parsing_prompt(job_text)
        response = self.invoke_with_json_validation(prompt, temperature=0.1)
        
        if not response.success:
            self.logger.error(f"Erreur parsing offre: {response.error}")
            return {"error": f"Ã‰chec du parsing: {response.error}"}
        
        try:
            job_data = json.loads(response.content)
            job_data["_metadata"] = {
                "parsing_time": response.execution_time,
                "tokens_used": response.tokens_used,
                "model_used": response.model_used,
                "confidence": self._calculate_parsing_confidence(job_data)
            }
            return job_data
            
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON invalide reÃ§u: {str(e)}")
            return {"error": f"JSON invalide: {str(e)}"}
    
    def _calculate_parsing_confidence(self, parsed_data: Dict) -> float:
        """Calcule un score de confiance du parsing (0-1)"""
        if not parsed_data or "error" in parsed_data:
            return 0.0
        
        confidence_factors = []
        
        # VÃ©rification des champs obligatoires
        if "titre_poste" in parsed_data or "titre_candidat" in parsed_data:
            title = parsed_data.get("titre_poste", parsed_data.get("titre_candidat", ""))
            confidence_factors.append(1.0 if len(title.strip()) > 5 else 0.5)
        
        # VÃ©rification de la richesse des donnÃ©es
        non_empty_fields = 0
        total_fields = 0
        
        for key, value in parsed_data.items():
            if key.startswith("_"):  # Skip metadata
                continue
                
            total_fields += 1
            if isinstance(value, str) and len(value.strip()) > 0:
                non_empty_fields += 1
            elif isinstance(value, list) and len(value) > 0:
                non_empty_fields += 1
            elif isinstance(value, dict) and any(v for v in value.values() if v):
                non_empty_fields += 1
            elif isinstance(value, (int, float)) and value > 0:
                non_empty_fields += 1
        
        completeness_score = non_empty_fields / total_fields if total_fields > 0 else 0
        confidence_factors.append(completeness_score)
        
        # Score final
        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.0
    
    def batch_parse_cvs(self, cv_texts: List[str]) -> List[Dict]:
        """Parse multiple CVs en lot"""
        results = []
        total = len(cv_texts)
        
        for i, cv_text in enumerate(cv_texts):
            self.logger.info(f"Parsing CV {i+1}/{total}")
            result = self.parse_cv_text(cv_text)
            result["_batch_index"] = i
            results.append(result)
            
            # Petite pause pour Ã©viter la surcharge
            time.sleep(0.5)
        
        return results
    
    def batch_parse_jobs(self, job_texts: List[str]) -> List[Dict]:
        """Parse multiple offres en lot"""
        results = []
        total = len(job_texts)
        
        for i, job_text in enumerate(job_texts):
            self.logger.info(f"Parsing offre {i+1}/{total}")
            result = self.parse_job_text(job_text)
            result["_batch_index"] = i
            results.append(result)
            
            # Petite pause pour Ã©viter la surcharge
            time.sleep(0.5)
        
        return results
    
    def get_stats(self) -> Dict:
        """Retourne les statistiques d'utilisation"""
        stats = self.stats.copy()
        stats["success_rate"] = (
            self.stats["successful_requests"] / self.stats["total_requests"] * 100
            if self.stats["total_requests"] > 0 else 0
        )
        stats["average_tokens_per_request"] = (
            self.stats["total_tokens"] / self.stats["successful_requests"]
            if self.stats["successful_requests"] > 0 else 0
        )
        return stats
    
    def reset_stats(self):
        """Remet Ã  zÃ©ro les statistiques"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "average_response_time": 0.0
        }
    
    def health_check(self) -> Dict:
        """VÃ©rifie l'Ã©tat de santÃ© du client"""
        try:
            start_time = time.time()
            test_response = self.invoke("Test de santÃ©: rÃ©ponds juste 'OK'")
            response_time = time.time() - start_time
            
            if test_response.success and "ok" in test_response.content.lower():
                return {
                    "status": "healthy",
                    "response_time": response_time,
                    "model": self.model_name,
                    "base_url": self.base_url
                }
            else:
                return {
                    "status": "unhealthy",
                    "error": test_response.error or "RÃ©ponse inattendue",
                    "response_time": response_time
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "response_time": None
            }
    
    def update_temperature(self, new_temperature: float):
        """Met Ã  jour la tempÃ©rature du modÃ¨le"""
        if 0.0 <= new_temperature <= 2.0:
            self.temperature = new_temperature
            self.llm.temperature = new_temperature
            self.logger.info(f"TempÃ©rature mise Ã  jour: {new_temperature}")
        else:
            raise ValueError("La tempÃ©rature doit Ãªtre entre 0.0 et 2.0")

# Fonction utilitaire pour usage simple
def quick_parse_cv(cv_text: str, model_name: str = "gemma2:9b") -> Dict:
    """Parse rapide d'un CV"""
    client = GemmaClient(model_name=model_name)
    return client.parse_cv_text(cv_text)

def quick_parse_job(job_text: str, model_name: str = "gemma2:9b") -> Dict:
    """Parse rapide d'une offre"""
    client = GemmaClient(model_name=model_name)
    return client.parse_job_text(job_text)

# Test de la classe
if __name__ == "__main__":
    print("ğŸ¤– Test du client Gemma3...")
    
    try:
        # Initialisation
        client = GemmaClient(model_name="gemma2:9b")
        
        # Test de santÃ©
        health = client.health_check()
        print(f"ğŸ“Š Ã‰tat de santÃ©: {health['status']}")
        
        if health["status"] == "healthy":
            # Test simple
            print("\nğŸ§ª Test d'invocation simple...")
            response = client.invoke("Explique en une phrase ce qu'est l'intelligence artificielle.")
            
            if response.success:
                print(f"âœ… RÃ©ponse reÃ§ue: {response.content[:100]}...")
                print(f"â±ï¸  Temps: {response.execution_time:.2f}s")
                print(f"ğŸ”¤ Tokens estimÃ©s: {response.tokens_used}")
            else:
                print(f"âŒ Ã‰chec: {response.error}")
            
            # Test parsing CV
            print("\nğŸ“„ Test parsing CV...")
            cv_exemple = """
            Jean Dupont
            DÃ©veloppeur Full Stack Senior
            
            ExpÃ©rience:
            - 2020-2023: Lead Developer chez TechCorp
            - 2018-2020: DÃ©veloppeur Python chez StartupXYZ
            
            Formation:
            - 2018: Master Informatique, UniversitÃ© Paris-Saclay
            
            CompÃ©tences: Python, JavaScript, React, Docker, AWS
            Langues: FranÃ§ais (natif), Anglais (courant)
            """
            
            cv_result = client.parse_cv_text(cv_exemple)
            
            if "error" not in cv_result:
                print("âœ… CV parsÃ© avec succÃ¨s!")
                print(f"ğŸ“‹ Titre: {cv_result.get('titre_candidat', 'N/A')}")
                print(f"ğŸ“ Formations: {len(cv_result.get('formations', []))}")
                print(f"ğŸ’¼ ExpÃ©riences: {len(cv_result.get('experience', []))}")
                print(f"ğŸ› ï¸  CompÃ©tences techniques: {len(cv_result.get('competences_techniques', []))}")
                print(f"ğŸ¯ Confiance: {cv_result.get('_metadata', {}).get('confidence', 0):.2f}")
            else:
                print(f"âŒ Erreur parsing CV: {cv_result['error']}")
            
            # Statistiques
            print(f"\nğŸ“Š Statistiques:")
            stats = client.get_stats()
            print(f"   RequÃªtes totales: {stats['total_requests']}")
            print(f"   Taux de succÃ¨s: {stats['success_rate']:.1f}%")
            print(f"   Temps moyen: {stats['average_response_time']:.2f}s")
            
        else:
            print(f"âŒ ProblÃ¨me de santÃ©: {health.get('error', 'Inconnu')}")
            
    except Exception as e:
        print(f"âŒ Erreur lors du test: {str(e)}")
        print("ğŸ’¡ VÃ©rifiez qu'Ollama est lancÃ© et que Gemma2 est installÃ©:")
        print("   ollama pull gemma2:9b")
        print("   ollama serve")